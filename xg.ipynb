{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa92af1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dedadc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "def read_train_file():\n",
    "    data=[]\n",
    "    trains=pd.read_csv('security_train.csv',engine='python',chunksize=10000)\n",
    "    for train in trains:\n",
    "        data.append(train)\n",
    "    print(\"Finished\")       \n",
    "    datas = pd.concat(data, ignore_index= True,axis=0)\n",
    "    labels = []\n",
    "    apis = []\n",
    "    group = datas.groupby('file_id') ##generate a groupby object                                                 \n",
    "    for key,one in group:\n",
    "        one_label = one['label'].values[0]                                      \n",
    "        result = one.sort_values(['tid', 'index'], ascending=True)   \n",
    "        one_api = ' '.join(result['api'])\n",
    "        labels.append(one_label)\n",
    "        apis.append(one_api)\n",
    "    with open(\"security_train.pkl\", 'wb') as f:\n",
    "        pickle.dump(labels, f)\n",
    "        pickle.dump(apis, f)\n",
    "read_train_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15fb1a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "def read_test_file():\n",
    "    data=[]\n",
    "    tests=pd.read_csv('security_test.csv',engine='python',chunksize=10000)\n",
    "    for test in tests:\n",
    "        data.append(test)\n",
    "    print(\"Finished\")       \n",
    "    datas = pd.concat(data, ignore_index= True,axis=0)\n",
    "    fileids = []\n",
    "    testapis = []\n",
    "    group = datas.groupby('file_id') ##generate a groupby object                                                 \n",
    "    for key,one in group:                                \n",
    "        result = one.sort_values(['tid', 'index'], ascending=True)   \n",
    "        one_api = ' '.join(result['api'])\n",
    "        fileids.append(key)\n",
    "        testapis.append(one_api)\n",
    "    with open(\"security_test.pkl\", 'wb') as f:\n",
    "        pickle.dump(fileids, f)\n",
    "        pickle.dump(testapis, f)\n",
    "read_test_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e023977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f22a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"security_train.pkl\", \"rb\") as f:\n",
    "    labels = pickle.load(f)\n",
    "    apis = pickle.load(f)\n",
    "with open(\"security_test.pkl\", \"rb\") as f:\n",
    "    fileids = pickle.load(f)\n",
    "    testapis = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac67e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "labels=pd.Series(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffcc8496",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.358465\n",
       "5    0.308850\n",
       "7    0.107079\n",
       "2    0.086124\n",
       "3    0.059048\n",
       "6    0.037085\n",
       "1    0.036149\n",
       "4    0.007201\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.value_counts() / labels.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word2vec.txt', 'a') as f:\n",
    "    for one in apis:\n",
    "        f.write(one)\n",
    "        f.write('\\n')\n",
    "    for one in testapis:\n",
    "        f.write(one)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a09cd7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "sentences = LineSentence('word2vec.txt')\n",
    "word2vec = Word2Vec(sentences,vector_size= 128, min_count=1)\n",
    "word2vec.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1845b653",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301\n"
     ]
    }
   ],
   "source": [
    "word2vec = Word2Vec.load('word2vec.model')\n",
    "print(len(word2vec.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41688ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "transform\n"
     ]
    }
   ],
   "source": [
    "##use Word2Vec and CountVectorizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "word2vec = Word2Vec.load('word2vec.model')\n",
    "\n",
    "train_word2vec = []\n",
    "test_word2vec = []\n",
    "\n",
    "for word in apis:\n",
    "    for key in word.split(' '): \n",
    "        one = []\n",
    "        one.extend(word2vec.wv[key])\n",
    "        mean = np.mean(one, axis=0)\n",
    "        train_word2vec.append(mean)\n",
    "for word in testapis:\n",
    "    for key in word.split(' '): \n",
    "        one = []\n",
    "        one.extend(word2vec.wv[key])\n",
    "        mean = np.mean(one, axis=0)\n",
    "        test_word2vec.append(mean)\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 4), min_df=0.100000, max_df=0.800000) \n",
    "train = vectorizer.fit_transform(apis)\n",
    "print(\"fit\")\n",
    "test = vectorizer.transform(testapis)\n",
    "print(\"transform\")\n",
    "train_word2vec = np.array(train_word2vec) \n",
    "test_word2vec = np.array(test_word2vec) \n",
    "train = train.A.astype('float32') \n",
    "test = test.A.astype('float32')\n",
    "a = np.concatenate((train, train_word2vec), axis=1) \n",
    "b = np.concatenate((test, test_word2vec), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c6041e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"word2.pkl\", 'wb') as f:\n",
    "    pickle.dump(train_word2vec, f)\n",
    "    pickle.dump(test_word2vec, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aca02b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "test_result = np.zeros(shape=(len(b), 8))\n",
    "skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e6b6e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0\n",
      "2778 11109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\xgboost\\core.py:525: FutureWarning: Pass `evals` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:18:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.79197\tval-mlogloss:1.79949\n",
      "[1]\ttrain-mlogloss:1.58026\tval-mlogloss:1.59566\n",
      "[2]\ttrain-mlogloss:1.41468\tval-mlogloss:1.43836\n",
      "[3]\ttrain-mlogloss:1.28048\tval-mlogloss:1.31159\n",
      "[4]\ttrain-mlogloss:1.16738\tval-mlogloss:1.20510\n",
      "[5]\ttrain-mlogloss:1.07081\tval-mlogloss:1.11407\n",
      "[6]\ttrain-mlogloss:0.98756\tval-mlogloss:1.03591\n",
      "[7]\ttrain-mlogloss:0.91440\tval-mlogloss:0.96754\n",
      "[8]\ttrain-mlogloss:0.84951\tval-mlogloss:0.90780\n",
      "[9]\ttrain-mlogloss:0.79080\tval-mlogloss:0.85355\n",
      "[10]\ttrain-mlogloss:0.73854\tval-mlogloss:0.80511\n",
      "[11]\ttrain-mlogloss:0.69282\tval-mlogloss:0.76349\n",
      "[12]\ttrain-mlogloss:0.65125\tval-mlogloss:0.72492\n",
      "[13]\ttrain-mlogloss:0.61318\tval-mlogloss:0.69103\n",
      "[14]\ttrain-mlogloss:0.57992\tval-mlogloss:0.66074\n",
      "[15]\ttrain-mlogloss:0.54839\tval-mlogloss:0.63312\n",
      "[16]\ttrain-mlogloss:0.51916\tval-mlogloss:0.60691\n",
      "[17]\ttrain-mlogloss:0.49218\tval-mlogloss:0.58286\n",
      "[18]\ttrain-mlogloss:0.46757\tval-mlogloss:0.56184\n",
      "[19]\ttrain-mlogloss:0.44553\tval-mlogloss:0.54314\n",
      "[20]\ttrain-mlogloss:0.42440\tval-mlogloss:0.52536\n",
      "[21]\ttrain-mlogloss:0.40523\tval-mlogloss:0.50941\n",
      "[22]\ttrain-mlogloss:0.38650\tval-mlogloss:0.49395\n",
      "[23]\ttrain-mlogloss:0.36949\tval-mlogloss:0.48030\n",
      "[24]\ttrain-mlogloss:0.35476\tval-mlogloss:0.46792\n",
      "[25]\ttrain-mlogloss:0.34037\tval-mlogloss:0.45659\n",
      "[26]\ttrain-mlogloss:0.32757\tval-mlogloss:0.44661\n",
      "[27]\ttrain-mlogloss:0.31507\tval-mlogloss:0.43679\n",
      "[28]\ttrain-mlogloss:0.30332\tval-mlogloss:0.42742\n",
      "[29]\ttrain-mlogloss:0.29209\tval-mlogloss:0.41941\n",
      "[30]\ttrain-mlogloss:0.28189\tval-mlogloss:0.41197\n",
      "[31]\ttrain-mlogloss:0.27182\tval-mlogloss:0.40525\n",
      "[32]\ttrain-mlogloss:0.26250\tval-mlogloss:0.39884\n",
      "[33]\ttrain-mlogloss:0.25441\tval-mlogloss:0.39322\n",
      "[34]\ttrain-mlogloss:0.24635\tval-mlogloss:0.38756\n",
      "[35]\ttrain-mlogloss:0.23888\tval-mlogloss:0.38302\n",
      "[36]\ttrain-mlogloss:0.23220\tval-mlogloss:0.37846\n",
      "[37]\ttrain-mlogloss:0.22576\tval-mlogloss:0.37458\n",
      "[38]\ttrain-mlogloss:0.21921\tval-mlogloss:0.37058\n",
      "[39]\ttrain-mlogloss:0.21263\tval-mlogloss:0.36645\n",
      "[40]\ttrain-mlogloss:0.20738\tval-mlogloss:0.36351\n",
      "[41]\ttrain-mlogloss:0.20226\tval-mlogloss:0.36104\n",
      "[42]\ttrain-mlogloss:0.19726\tval-mlogloss:0.35845\n",
      "[43]\ttrain-mlogloss:0.19235\tval-mlogloss:0.35609\n",
      "[44]\ttrain-mlogloss:0.18749\tval-mlogloss:0.35312\n",
      "[45]\ttrain-mlogloss:0.18294\tval-mlogloss:0.35091\n",
      "[46]\ttrain-mlogloss:0.17843\tval-mlogloss:0.34823\n",
      "[47]\ttrain-mlogloss:0.17436\tval-mlogloss:0.34593\n",
      "[48]\ttrain-mlogloss:0.17045\tval-mlogloss:0.34396\n",
      "[49]\ttrain-mlogloss:0.16697\tval-mlogloss:0.34219\n",
      "[50]\ttrain-mlogloss:0.16404\tval-mlogloss:0.34085\n",
      "[51]\ttrain-mlogloss:0.16093\tval-mlogloss:0.33927\n",
      "[52]\ttrain-mlogloss:0.15719\tval-mlogloss:0.33751\n",
      "[53]\ttrain-mlogloss:0.15423\tval-mlogloss:0.33651\n",
      "[54]\ttrain-mlogloss:0.15160\tval-mlogloss:0.33542\n",
      "[55]\ttrain-mlogloss:0.14928\tval-mlogloss:0.33423\n",
      "[56]\ttrain-mlogloss:0.14635\tval-mlogloss:0.33317\n",
      "[57]\ttrain-mlogloss:0.14359\tval-mlogloss:0.33230\n",
      "[58]\ttrain-mlogloss:0.14099\tval-mlogloss:0.33134\n",
      "[59]\ttrain-mlogloss:0.13879\tval-mlogloss:0.33038\n",
      "[60]\ttrain-mlogloss:0.13631\tval-mlogloss:0.32985\n",
      "[61]\ttrain-mlogloss:0.13368\tval-mlogloss:0.32901\n",
      "[62]\ttrain-mlogloss:0.13101\tval-mlogloss:0.32840\n",
      "[63]\ttrain-mlogloss:0.12855\tval-mlogloss:0.32792\n",
      "[64]\ttrain-mlogloss:0.12620\tval-mlogloss:0.32711\n",
      "[65]\ttrain-mlogloss:0.12386\tval-mlogloss:0.32603\n",
      "[66]\ttrain-mlogloss:0.12178\tval-mlogloss:0.32548\n",
      "[67]\ttrain-mlogloss:0.11989\tval-mlogloss:0.32497\n",
      "[68]\ttrain-mlogloss:0.11821\tval-mlogloss:0.32457\n",
      "[69]\ttrain-mlogloss:0.11623\tval-mlogloss:0.32419\n",
      "[70]\ttrain-mlogloss:0.11446\tval-mlogloss:0.32363\n",
      "[71]\ttrain-mlogloss:0.11267\tval-mlogloss:0.32339\n",
      "[72]\ttrain-mlogloss:0.11052\tval-mlogloss:0.32242\n",
      "[73]\ttrain-mlogloss:0.10867\tval-mlogloss:0.32170\n",
      "[74]\ttrain-mlogloss:0.10700\tval-mlogloss:0.32139\n",
      "[75]\ttrain-mlogloss:0.10557\tval-mlogloss:0.32090\n",
      "[76]\ttrain-mlogloss:0.10414\tval-mlogloss:0.32050\n",
      "[77]\ttrain-mlogloss:0.10250\tval-mlogloss:0.32037\n",
      "[78]\ttrain-mlogloss:0.10101\tval-mlogloss:0.31970\n",
      "[79]\ttrain-mlogloss:0.09962\tval-mlogloss:0.31940\n",
      "[80]\ttrain-mlogloss:0.09831\tval-mlogloss:0.31953\n",
      "[81]\ttrain-mlogloss:0.09681\tval-mlogloss:0.31945\n",
      "[82]\ttrain-mlogloss:0.09529\tval-mlogloss:0.31946\n",
      "[83]\ttrain-mlogloss:0.09379\tval-mlogloss:0.31914\n",
      "[84]\ttrain-mlogloss:0.09255\tval-mlogloss:0.31866\n",
      "[85]\ttrain-mlogloss:0.09122\tval-mlogloss:0.31859\n",
      "[86]\ttrain-mlogloss:0.08987\tval-mlogloss:0.31852\n",
      "[87]\ttrain-mlogloss:0.08872\tval-mlogloss:0.31845\n",
      "[88]\ttrain-mlogloss:0.08749\tval-mlogloss:0.31823\n",
      "[89]\ttrain-mlogloss:0.08627\tval-mlogloss:0.31839\n",
      "[90]\ttrain-mlogloss:0.08536\tval-mlogloss:0.31802\n",
      "[91]\ttrain-mlogloss:0.08422\tval-mlogloss:0.31798\n",
      "[92]\ttrain-mlogloss:0.08303\tval-mlogloss:0.31775\n",
      "[93]\ttrain-mlogloss:0.08197\tval-mlogloss:0.31788\n",
      "[94]\ttrain-mlogloss:0.08084\tval-mlogloss:0.31776\n",
      "[95]\ttrain-mlogloss:0.07984\tval-mlogloss:0.31776\n",
      "[96]\ttrain-mlogloss:0.07895\tval-mlogloss:0.31757\n",
      "[97]\ttrain-mlogloss:0.07807\tval-mlogloss:0.31756\n",
      "[98]\ttrain-mlogloss:0.07699\tval-mlogloss:0.31772\n",
      "[99]\ttrain-mlogloss:0.07597\tval-mlogloss:0.31801\n",
      "[100]\ttrain-mlogloss:0.07499\tval-mlogloss:0.31787\n",
      "[101]\ttrain-mlogloss:0.07405\tval-mlogloss:0.31775\n",
      "[102]\ttrain-mlogloss:0.07329\tval-mlogloss:0.31780\n",
      "[103]\ttrain-mlogloss:0.07234\tval-mlogloss:0.31813\n",
      "[104]\ttrain-mlogloss:0.07168\tval-mlogloss:0.31792\n",
      "[105]\ttrain-mlogloss:0.07099\tval-mlogloss:0.31823\n",
      "[106]\ttrain-mlogloss:0.07019\tval-mlogloss:0.31819\n",
      "[107]\ttrain-mlogloss:0.06932\tval-mlogloss:0.31796\n",
      "[108]\ttrain-mlogloss:0.06842\tval-mlogloss:0.31813\n",
      "[109]\ttrain-mlogloss:0.06769\tval-mlogloss:0.31834\n",
      "[110]\ttrain-mlogloss:0.06693\tval-mlogloss:0.31835\n",
      "[111]\ttrain-mlogloss:0.06626\tval-mlogloss:0.31854\n",
      "[112]\ttrain-mlogloss:0.06550\tval-mlogloss:0.31875\n",
      "[113]\ttrain-mlogloss:0.06479\tval-mlogloss:0.31869\n",
      "[114]\ttrain-mlogloss:0.06408\tval-mlogloss:0.31892\n",
      "[115]\ttrain-mlogloss:0.06334\tval-mlogloss:0.31880\n",
      "[116]\ttrain-mlogloss:0.06266\tval-mlogloss:0.31885\n",
      "[117]\ttrain-mlogloss:0.06184\tval-mlogloss:0.31863\n",
      "[118]\ttrain-mlogloss:0.06130\tval-mlogloss:0.31885\n",
      "[119]\ttrain-mlogloss:0.06065\tval-mlogloss:0.31895\n",
      "[120]\ttrain-mlogloss:0.05994\tval-mlogloss:0.31916\n",
      "[121]\ttrain-mlogloss:0.05935\tval-mlogloss:0.31925\n",
      "[122]\ttrain-mlogloss:0.05871\tval-mlogloss:0.31951\n",
      "[123]\ttrain-mlogloss:0.05826\tval-mlogloss:0.31959\n",
      "[124]\ttrain-mlogloss:0.05771\tval-mlogloss:0.31995\n",
      "[125]\ttrain-mlogloss:0.05715\tval-mlogloss:0.32022\n",
      "[126]\ttrain-mlogloss:0.05665\tval-mlogloss:0.32036\n",
      "[127]\ttrain-mlogloss:0.05599\tval-mlogloss:0.32052\n",
      "[128]\ttrain-mlogloss:0.05539\tval-mlogloss:0.32078\n",
      "[129]\ttrain-mlogloss:0.05479\tval-mlogloss:0.32099\n",
      "[130]\ttrain-mlogloss:0.05429\tval-mlogloss:0.32126\n",
      "[131]\ttrain-mlogloss:0.05377\tval-mlogloss:0.32138\n",
      "[132]\ttrain-mlogloss:0.05310\tval-mlogloss:0.32167\n",
      "[133]\ttrain-mlogloss:0.05248\tval-mlogloss:0.32204\n",
      "[134]\ttrain-mlogloss:0.05203\tval-mlogloss:0.32213\n",
      "[135]\ttrain-mlogloss:0.05151\tval-mlogloss:0.32247\n",
      "[136]\ttrain-mlogloss:0.05104\tval-mlogloss:0.32239\n",
      "[137]\ttrain-mlogloss:0.05053\tval-mlogloss:0.32243\n",
      "[138]\ttrain-mlogloss:0.04992\tval-mlogloss:0.32258\n",
      "[139]\ttrain-mlogloss:0.04946\tval-mlogloss:0.32271\n",
      "[140]\ttrain-mlogloss:0.04900\tval-mlogloss:0.32314\n",
      "[141]\ttrain-mlogloss:0.04859\tval-mlogloss:0.32336\n",
      "[142]\ttrain-mlogloss:0.04817\tval-mlogloss:0.32381\n",
      "[143]\ttrain-mlogloss:0.04785\tval-mlogloss:0.32386\n",
      "[144]\ttrain-mlogloss:0.04746\tval-mlogloss:0.32409\n",
      "[145]\ttrain-mlogloss:0.04718\tval-mlogloss:0.32451\n",
      "[146]\ttrain-mlogloss:0.04680\tval-mlogloss:0.32463\n",
      "FOLD: 1\n",
      "2778 11109\n",
      "[15:27:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.78940\tval-mlogloss:1.80621\n",
      "[1]\ttrain-mlogloss:1.57774\tval-mlogloss:1.60747\n",
      "[2]\ttrain-mlogloss:1.41219\tval-mlogloss:1.45294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\ttrain-mlogloss:1.27849\tval-mlogloss:1.32688\n",
      "[4]\ttrain-mlogloss:1.16503\tval-mlogloss:1.22251\n",
      "[5]\ttrain-mlogloss:1.06787\tval-mlogloss:1.13355\n",
      "[6]\ttrain-mlogloss:0.98399\tval-mlogloss:1.05665\n",
      "[7]\ttrain-mlogloss:0.91070\tval-mlogloss:0.98940\n",
      "[8]\ttrain-mlogloss:0.84511\tval-mlogloss:0.92945\n",
      "[9]\ttrain-mlogloss:0.78700\tval-mlogloss:0.87600\n",
      "[10]\ttrain-mlogloss:0.73497\tval-mlogloss:0.82927\n",
      "[11]\ttrain-mlogloss:0.68826\tval-mlogloss:0.78844\n",
      "[12]\ttrain-mlogloss:0.64650\tval-mlogloss:0.75097\n",
      "[13]\ttrain-mlogloss:0.60851\tval-mlogloss:0.71697\n",
      "[14]\ttrain-mlogloss:0.57431\tval-mlogloss:0.68726\n",
      "[15]\ttrain-mlogloss:0.54306\tval-mlogloss:0.65932\n",
      "[16]\ttrain-mlogloss:0.51450\tval-mlogloss:0.63426\n",
      "[17]\ttrain-mlogloss:0.48766\tval-mlogloss:0.61088\n",
      "[18]\ttrain-mlogloss:0.46409\tval-mlogloss:0.59044\n",
      "[19]\ttrain-mlogloss:0.44182\tval-mlogloss:0.57156\n",
      "[20]\ttrain-mlogloss:0.42047\tval-mlogloss:0.55376\n",
      "[21]\ttrain-mlogloss:0.40086\tval-mlogloss:0.53778\n",
      "[22]\ttrain-mlogloss:0.38308\tval-mlogloss:0.52289\n",
      "[23]\ttrain-mlogloss:0.36599\tval-mlogloss:0.50915\n",
      "[24]\ttrain-mlogloss:0.35057\tval-mlogloss:0.49720\n",
      "[25]\ttrain-mlogloss:0.33525\tval-mlogloss:0.48524\n",
      "[26]\ttrain-mlogloss:0.32215\tval-mlogloss:0.47464\n",
      "[27]\ttrain-mlogloss:0.30939\tval-mlogloss:0.46514\n",
      "[28]\ttrain-mlogloss:0.29816\tval-mlogloss:0.45605\n",
      "[29]\ttrain-mlogloss:0.28698\tval-mlogloss:0.44768\n",
      "[30]\ttrain-mlogloss:0.27602\tval-mlogloss:0.44011\n",
      "[31]\ttrain-mlogloss:0.26643\tval-mlogloss:0.43416\n",
      "[32]\ttrain-mlogloss:0.25726\tval-mlogloss:0.42796\n",
      "[33]\ttrain-mlogloss:0.24855\tval-mlogloss:0.42204\n",
      "[34]\ttrain-mlogloss:0.24041\tval-mlogloss:0.41720\n",
      "[35]\ttrain-mlogloss:0.23330\tval-mlogloss:0.41266\n",
      "[36]\ttrain-mlogloss:0.22662\tval-mlogloss:0.40804\n",
      "[37]\ttrain-mlogloss:0.21992\tval-mlogloss:0.40357\n",
      "[38]\ttrain-mlogloss:0.21387\tval-mlogloss:0.39975\n",
      "[39]\ttrain-mlogloss:0.20784\tval-mlogloss:0.39570\n",
      "[40]\ttrain-mlogloss:0.20267\tval-mlogloss:0.39245\n",
      "[41]\ttrain-mlogloss:0.19752\tval-mlogloss:0.38937\n",
      "[42]\ttrain-mlogloss:0.19215\tval-mlogloss:0.38642\n",
      "[43]\ttrain-mlogloss:0.18752\tval-mlogloss:0.38345\n",
      "[44]\ttrain-mlogloss:0.18273\tval-mlogloss:0.38065\n",
      "[45]\ttrain-mlogloss:0.17854\tval-mlogloss:0.37799\n",
      "[46]\ttrain-mlogloss:0.17456\tval-mlogloss:0.37527\n",
      "[47]\ttrain-mlogloss:0.17083\tval-mlogloss:0.37359\n",
      "[48]\ttrain-mlogloss:0.16682\tval-mlogloss:0.37120\n",
      "[49]\ttrain-mlogloss:0.16349\tval-mlogloss:0.36917\n",
      "[50]\ttrain-mlogloss:0.15995\tval-mlogloss:0.36716\n",
      "[51]\ttrain-mlogloss:0.15614\tval-mlogloss:0.36552\n",
      "[52]\ttrain-mlogloss:0.15280\tval-mlogloss:0.36413\n",
      "[53]\ttrain-mlogloss:0.14948\tval-mlogloss:0.36259\n",
      "[54]\ttrain-mlogloss:0.14654\tval-mlogloss:0.36126\n",
      "[55]\ttrain-mlogloss:0.14332\tval-mlogloss:0.36008\n",
      "[56]\ttrain-mlogloss:0.14089\tval-mlogloss:0.35855\n",
      "[57]\ttrain-mlogloss:0.13821\tval-mlogloss:0.35736\n",
      "[58]\ttrain-mlogloss:0.13526\tval-mlogloss:0.35586\n",
      "[59]\ttrain-mlogloss:0.13309\tval-mlogloss:0.35497\n",
      "[60]\ttrain-mlogloss:0.13071\tval-mlogloss:0.35392\n",
      "[61]\ttrain-mlogloss:0.12840\tval-mlogloss:0.35301\n",
      "[62]\ttrain-mlogloss:0.12638\tval-mlogloss:0.35222\n",
      "[63]\ttrain-mlogloss:0.12440\tval-mlogloss:0.35140\n",
      "[64]\ttrain-mlogloss:0.12204\tval-mlogloss:0.35022\n",
      "[65]\ttrain-mlogloss:0.11999\tval-mlogloss:0.34955\n",
      "[66]\ttrain-mlogloss:0.11798\tval-mlogloss:0.34848\n",
      "[67]\ttrain-mlogloss:0.11630\tval-mlogloss:0.34747\n",
      "[68]\ttrain-mlogloss:0.11454\tval-mlogloss:0.34668\n",
      "[69]\ttrain-mlogloss:0.11259\tval-mlogloss:0.34583\n",
      "[70]\ttrain-mlogloss:0.11091\tval-mlogloss:0.34556\n",
      "[71]\ttrain-mlogloss:0.10908\tval-mlogloss:0.34479\n",
      "[72]\ttrain-mlogloss:0.10731\tval-mlogloss:0.34459\n",
      "[73]\ttrain-mlogloss:0.10550\tval-mlogloss:0.34391\n",
      "[74]\ttrain-mlogloss:0.10390\tval-mlogloss:0.34368\n",
      "[75]\ttrain-mlogloss:0.10235\tval-mlogloss:0.34325\n",
      "[76]\ttrain-mlogloss:0.10090\tval-mlogloss:0.34289\n",
      "[77]\ttrain-mlogloss:0.09930\tval-mlogloss:0.34272\n",
      "[78]\ttrain-mlogloss:0.09817\tval-mlogloss:0.34238\n",
      "[79]\ttrain-mlogloss:0.09690\tval-mlogloss:0.34206\n",
      "[80]\ttrain-mlogloss:0.09523\tval-mlogloss:0.34171\n",
      "[81]\ttrain-mlogloss:0.09384\tval-mlogloss:0.34121\n",
      "[82]\ttrain-mlogloss:0.09255\tval-mlogloss:0.34089\n",
      "[83]\ttrain-mlogloss:0.09134\tval-mlogloss:0.34025\n",
      "[84]\ttrain-mlogloss:0.09002\tval-mlogloss:0.34016\n",
      "[85]\ttrain-mlogloss:0.08888\tval-mlogloss:0.34001\n",
      "[86]\ttrain-mlogloss:0.08785\tval-mlogloss:0.33984\n",
      "[87]\ttrain-mlogloss:0.08658\tval-mlogloss:0.33976\n",
      "[88]\ttrain-mlogloss:0.08533\tval-mlogloss:0.33955\n",
      "[89]\ttrain-mlogloss:0.08413\tval-mlogloss:0.33935\n",
      "[90]\ttrain-mlogloss:0.08298\tval-mlogloss:0.33939\n",
      "[91]\ttrain-mlogloss:0.08176\tval-mlogloss:0.33919\n",
      "[92]\ttrain-mlogloss:0.08066\tval-mlogloss:0.33941\n",
      "[93]\ttrain-mlogloss:0.07945\tval-mlogloss:0.33938\n",
      "[94]\ttrain-mlogloss:0.07877\tval-mlogloss:0.33897\n",
      "[95]\ttrain-mlogloss:0.07780\tval-mlogloss:0.33866\n",
      "[96]\ttrain-mlogloss:0.07674\tval-mlogloss:0.33861\n",
      "[97]\ttrain-mlogloss:0.07590\tval-mlogloss:0.33896\n",
      "[98]\ttrain-mlogloss:0.07515\tval-mlogloss:0.33879\n",
      "[99]\ttrain-mlogloss:0.07428\tval-mlogloss:0.33890\n",
      "[100]\ttrain-mlogloss:0.07346\tval-mlogloss:0.33873\n",
      "[101]\ttrain-mlogloss:0.07238\tval-mlogloss:0.33885\n",
      "[102]\ttrain-mlogloss:0.07160\tval-mlogloss:0.33853\n",
      "[103]\ttrain-mlogloss:0.07082\tval-mlogloss:0.33829\n",
      "[104]\ttrain-mlogloss:0.06989\tval-mlogloss:0.33817\n",
      "[105]\ttrain-mlogloss:0.06909\tval-mlogloss:0.33825\n",
      "[106]\ttrain-mlogloss:0.06824\tval-mlogloss:0.33804\n",
      "[107]\ttrain-mlogloss:0.06730\tval-mlogloss:0.33828\n",
      "[108]\ttrain-mlogloss:0.06655\tval-mlogloss:0.33813\n",
      "[109]\ttrain-mlogloss:0.06577\tval-mlogloss:0.33799\n",
      "[110]\ttrain-mlogloss:0.06499\tval-mlogloss:0.33820\n",
      "[111]\ttrain-mlogloss:0.06434\tval-mlogloss:0.33820\n",
      "[112]\ttrain-mlogloss:0.06372\tval-mlogloss:0.33829\n",
      "[113]\ttrain-mlogloss:0.06302\tval-mlogloss:0.33784\n",
      "[114]\ttrain-mlogloss:0.06216\tval-mlogloss:0.33787\n",
      "[115]\ttrain-mlogloss:0.06151\tval-mlogloss:0.33806\n",
      "[116]\ttrain-mlogloss:0.06077\tval-mlogloss:0.33792\n",
      "[117]\ttrain-mlogloss:0.06012\tval-mlogloss:0.33817\n",
      "[118]\ttrain-mlogloss:0.05950\tval-mlogloss:0.33843\n",
      "[119]\ttrain-mlogloss:0.05890\tval-mlogloss:0.33829\n",
      "[120]\ttrain-mlogloss:0.05829\tval-mlogloss:0.33831\n",
      "[121]\ttrain-mlogloss:0.05760\tval-mlogloss:0.33840\n",
      "[122]\ttrain-mlogloss:0.05707\tval-mlogloss:0.33808\n",
      "[123]\ttrain-mlogloss:0.05654\tval-mlogloss:0.33825\n",
      "[124]\ttrain-mlogloss:0.05587\tval-mlogloss:0.33829\n",
      "[125]\ttrain-mlogloss:0.05512\tval-mlogloss:0.33849\n",
      "[126]\ttrain-mlogloss:0.05457\tval-mlogloss:0.33866\n",
      "[127]\ttrain-mlogloss:0.05408\tval-mlogloss:0.33893\n",
      "[128]\ttrain-mlogloss:0.05352\tval-mlogloss:0.33945\n",
      "[129]\ttrain-mlogloss:0.05300\tval-mlogloss:0.33949\n",
      "[130]\ttrain-mlogloss:0.05255\tval-mlogloss:0.33949\n",
      "[131]\ttrain-mlogloss:0.05206\tval-mlogloss:0.33964\n",
      "[132]\ttrain-mlogloss:0.05146\tval-mlogloss:0.33989\n",
      "[133]\ttrain-mlogloss:0.05102\tval-mlogloss:0.34023\n",
      "[134]\ttrain-mlogloss:0.05056\tval-mlogloss:0.34044\n",
      "[135]\ttrain-mlogloss:0.04992\tval-mlogloss:0.34103\n",
      "[136]\ttrain-mlogloss:0.04936\tval-mlogloss:0.34072\n",
      "[137]\ttrain-mlogloss:0.04890\tval-mlogloss:0.34119\n",
      "[138]\ttrain-mlogloss:0.04848\tval-mlogloss:0.34134\n",
      "[139]\ttrain-mlogloss:0.04806\tval-mlogloss:0.34156\n",
      "[140]\ttrain-mlogloss:0.04756\tval-mlogloss:0.34164\n",
      "[141]\ttrain-mlogloss:0.04717\tval-mlogloss:0.34174\n",
      "[142]\ttrain-mlogloss:0.04677\tval-mlogloss:0.34184\n",
      "[143]\ttrain-mlogloss:0.04640\tval-mlogloss:0.34207\n",
      "[144]\ttrain-mlogloss:0.04597\tval-mlogloss:0.34223\n",
      "[145]\ttrain-mlogloss:0.04553\tval-mlogloss:0.34244\n",
      "[146]\ttrain-mlogloss:0.04510\tval-mlogloss:0.34251\n",
      "[147]\ttrain-mlogloss:0.04470\tval-mlogloss:0.34266\n",
      "[148]\ttrain-mlogloss:0.04431\tval-mlogloss:0.34273\n",
      "[149]\ttrain-mlogloss:0.04393\tval-mlogloss:0.34310\n",
      "[150]\ttrain-mlogloss:0.04366\tval-mlogloss:0.34316\n",
      "[151]\ttrain-mlogloss:0.04337\tval-mlogloss:0.34337\n",
      "[152]\ttrain-mlogloss:0.04303\tval-mlogloss:0.34337\n",
      "[153]\ttrain-mlogloss:0.04265\tval-mlogloss:0.34357\n",
      "[154]\ttrain-mlogloss:0.04221\tval-mlogloss:0.34370\n",
      "[155]\ttrain-mlogloss:0.04186\tval-mlogloss:0.34392\n",
      "[156]\ttrain-mlogloss:0.04151\tval-mlogloss:0.34399\n",
      "[157]\ttrain-mlogloss:0.04111\tval-mlogloss:0.34425\n",
      "[158]\ttrain-mlogloss:0.04069\tval-mlogloss:0.34445\n",
      "[159]\ttrain-mlogloss:0.04031\tval-mlogloss:0.34476\n",
      "[160]\ttrain-mlogloss:0.03990\tval-mlogloss:0.34482\n",
      "[161]\ttrain-mlogloss:0.03958\tval-mlogloss:0.34508\n",
      "[162]\ttrain-mlogloss:0.03927\tval-mlogloss:0.34498\n",
      "FOLD: 2\n",
      "2777 11110\n",
      "[15:38:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.79266\tval-mlogloss:1.80241\n",
      "[1]\ttrain-mlogloss:1.58207\tval-mlogloss:1.59826\n",
      "[2]\ttrain-mlogloss:1.41654\tval-mlogloss:1.44006\n",
      "[3]\ttrain-mlogloss:1.28187\tval-mlogloss:1.31321\n",
      "[4]\ttrain-mlogloss:1.17235\tval-mlogloss:1.21084\n",
      "[5]\ttrain-mlogloss:1.07434\tval-mlogloss:1.11912\n",
      "[6]\ttrain-mlogloss:0.99159\tval-mlogloss:1.04178\n",
      "[7]\ttrain-mlogloss:0.91858\tval-mlogloss:0.97358\n",
      "[8]\ttrain-mlogloss:0.85310\tval-mlogloss:0.91293\n",
      "[9]\ttrain-mlogloss:0.79501\tval-mlogloss:0.85854\n",
      "[10]\ttrain-mlogloss:0.74345\tval-mlogloss:0.81084\n",
      "[11]\ttrain-mlogloss:0.69657\tval-mlogloss:0.76861\n",
      "[12]\ttrain-mlogloss:0.65489\tval-mlogloss:0.73100\n",
      "[13]\ttrain-mlogloss:0.61676\tval-mlogloss:0.69590\n",
      "[14]\ttrain-mlogloss:0.58155\tval-mlogloss:0.66487\n",
      "[15]\ttrain-mlogloss:0.54999\tval-mlogloss:0.63595\n",
      "[16]\ttrain-mlogloss:0.52109\tval-mlogloss:0.60987\n",
      "[17]\ttrain-mlogloss:0.49400\tval-mlogloss:0.58638\n",
      "[18]\ttrain-mlogloss:0.46852\tval-mlogloss:0.56438\n",
      "[19]\ttrain-mlogloss:0.44561\tval-mlogloss:0.54508\n",
      "[20]\ttrain-mlogloss:0.42536\tval-mlogloss:0.52782\n",
      "[21]\ttrain-mlogloss:0.40518\tval-mlogloss:0.51128\n",
      "[22]\ttrain-mlogloss:0.38763\tval-mlogloss:0.49625\n",
      "[23]\ttrain-mlogloss:0.37107\tval-mlogloss:0.48324\n",
      "[24]\ttrain-mlogloss:0.35598\tval-mlogloss:0.47114\n",
      "[25]\ttrain-mlogloss:0.34099\tval-mlogloss:0.45903\n",
      "[26]\ttrain-mlogloss:0.32721\tval-mlogloss:0.44856\n",
      "[27]\ttrain-mlogloss:0.31495\tval-mlogloss:0.43916\n",
      "[28]\ttrain-mlogloss:0.30383\tval-mlogloss:0.43027\n",
      "[29]\ttrain-mlogloss:0.29263\tval-mlogloss:0.42246\n",
      "[30]\ttrain-mlogloss:0.28282\tval-mlogloss:0.41488\n",
      "[31]\ttrain-mlogloss:0.27354\tval-mlogloss:0.40788\n",
      "[32]\ttrain-mlogloss:0.26492\tval-mlogloss:0.40185\n",
      "[33]\ttrain-mlogloss:0.25621\tval-mlogloss:0.39594\n",
      "[34]\ttrain-mlogloss:0.24754\tval-mlogloss:0.39014\n",
      "[35]\ttrain-mlogloss:0.24015\tval-mlogloss:0.38481\n",
      "[36]\ttrain-mlogloss:0.23288\tval-mlogloss:0.37996\n",
      "[37]\ttrain-mlogloss:0.22594\tval-mlogloss:0.37549\n",
      "[38]\ttrain-mlogloss:0.21939\tval-mlogloss:0.37101\n",
      "[39]\ttrain-mlogloss:0.21333\tval-mlogloss:0.36707\n",
      "[40]\ttrain-mlogloss:0.20803\tval-mlogloss:0.36303\n",
      "[41]\ttrain-mlogloss:0.20271\tval-mlogloss:0.35965\n",
      "[42]\ttrain-mlogloss:0.19760\tval-mlogloss:0.35681\n",
      "[43]\ttrain-mlogloss:0.19306\tval-mlogloss:0.35357\n",
      "[44]\ttrain-mlogloss:0.18857\tval-mlogloss:0.35099\n",
      "[45]\ttrain-mlogloss:0.18404\tval-mlogloss:0.34810\n",
      "[46]\ttrain-mlogloss:0.17927\tval-mlogloss:0.34556\n",
      "[47]\ttrain-mlogloss:0.17531\tval-mlogloss:0.34336\n",
      "[48]\ttrain-mlogloss:0.17178\tval-mlogloss:0.34140\n",
      "[49]\ttrain-mlogloss:0.16804\tval-mlogloss:0.33927\n",
      "[50]\ttrain-mlogloss:0.16428\tval-mlogloss:0.33732\n",
      "[51]\ttrain-mlogloss:0.16118\tval-mlogloss:0.33560\n",
      "[52]\ttrain-mlogloss:0.15776\tval-mlogloss:0.33427\n",
      "[53]\ttrain-mlogloss:0.15483\tval-mlogloss:0.33269\n",
      "[54]\ttrain-mlogloss:0.15199\tval-mlogloss:0.33171\n",
      "[55]\ttrain-mlogloss:0.14894\tval-mlogloss:0.33022\n",
      "[56]\ttrain-mlogloss:0.14635\tval-mlogloss:0.32881\n",
      "[57]\ttrain-mlogloss:0.14362\tval-mlogloss:0.32808\n",
      "[58]\ttrain-mlogloss:0.14113\tval-mlogloss:0.32663\n",
      "[59]\ttrain-mlogloss:0.13863\tval-mlogloss:0.32553\n",
      "[60]\ttrain-mlogloss:0.13635\tval-mlogloss:0.32434\n",
      "[61]\ttrain-mlogloss:0.13416\tval-mlogloss:0.32349\n",
      "[62]\ttrain-mlogloss:0.13184\tval-mlogloss:0.32260\n",
      "[63]\ttrain-mlogloss:0.12963\tval-mlogloss:0.32168\n",
      "[64]\ttrain-mlogloss:0.12718\tval-mlogloss:0.32107\n",
      "[65]\ttrain-mlogloss:0.12481\tval-mlogloss:0.32046\n",
      "[66]\ttrain-mlogloss:0.12264\tval-mlogloss:0.31945\n",
      "[67]\ttrain-mlogloss:0.12025\tval-mlogloss:0.31850\n",
      "[68]\ttrain-mlogloss:0.11830\tval-mlogloss:0.31813\n",
      "[69]\ttrain-mlogloss:0.11638\tval-mlogloss:0.31736\n",
      "[70]\ttrain-mlogloss:0.11508\tval-mlogloss:0.31701\n",
      "[71]\ttrain-mlogloss:0.11339\tval-mlogloss:0.31664\n",
      "[72]\ttrain-mlogloss:0.11191\tval-mlogloss:0.31616\n",
      "[73]\ttrain-mlogloss:0.11020\tval-mlogloss:0.31555\n",
      "[74]\ttrain-mlogloss:0.10891\tval-mlogloss:0.31500\n",
      "[75]\ttrain-mlogloss:0.10746\tval-mlogloss:0.31458\n",
      "[76]\ttrain-mlogloss:0.10593\tval-mlogloss:0.31401\n",
      "[77]\ttrain-mlogloss:0.10446\tval-mlogloss:0.31370\n",
      "[78]\ttrain-mlogloss:0.10302\tval-mlogloss:0.31349\n",
      "[79]\ttrain-mlogloss:0.10132\tval-mlogloss:0.31306\n",
      "[80]\ttrain-mlogloss:0.10004\tval-mlogloss:0.31258\n",
      "[81]\ttrain-mlogloss:0.09907\tval-mlogloss:0.31241\n",
      "[82]\ttrain-mlogloss:0.09761\tval-mlogloss:0.31175\n",
      "[83]\ttrain-mlogloss:0.09599\tval-mlogloss:0.31123\n",
      "[84]\ttrain-mlogloss:0.09447\tval-mlogloss:0.31068\n",
      "[85]\ttrain-mlogloss:0.09346\tval-mlogloss:0.31051\n",
      "[86]\ttrain-mlogloss:0.09218\tval-mlogloss:0.31033\n",
      "[87]\ttrain-mlogloss:0.09098\tval-mlogloss:0.31009\n",
      "[88]\ttrain-mlogloss:0.08964\tval-mlogloss:0.31009\n",
      "[89]\ttrain-mlogloss:0.08856\tval-mlogloss:0.30990\n",
      "[90]\ttrain-mlogloss:0.08764\tval-mlogloss:0.30976\n",
      "[91]\ttrain-mlogloss:0.08647\tval-mlogloss:0.30989\n",
      "[92]\ttrain-mlogloss:0.08538\tval-mlogloss:0.30956\n",
      "[93]\ttrain-mlogloss:0.08435\tval-mlogloss:0.30919\n",
      "[94]\ttrain-mlogloss:0.08350\tval-mlogloss:0.30884\n",
      "[95]\ttrain-mlogloss:0.08245\tval-mlogloss:0.30878\n",
      "[96]\ttrain-mlogloss:0.08145\tval-mlogloss:0.30865\n",
      "[97]\ttrain-mlogloss:0.08048\tval-mlogloss:0.30865\n",
      "[98]\ttrain-mlogloss:0.07957\tval-mlogloss:0.30871\n",
      "[99]\ttrain-mlogloss:0.07872\tval-mlogloss:0.30897\n",
      "[100]\ttrain-mlogloss:0.07764\tval-mlogloss:0.30907\n",
      "[101]\ttrain-mlogloss:0.07673\tval-mlogloss:0.30906\n",
      "[102]\ttrain-mlogloss:0.07594\tval-mlogloss:0.30900\n",
      "[103]\ttrain-mlogloss:0.07513\tval-mlogloss:0.30889\n",
      "[104]\ttrain-mlogloss:0.07424\tval-mlogloss:0.30893\n",
      "[105]\ttrain-mlogloss:0.07340\tval-mlogloss:0.30922\n",
      "[106]\ttrain-mlogloss:0.07248\tval-mlogloss:0.30942\n",
      "[107]\ttrain-mlogloss:0.07169\tval-mlogloss:0.30944\n",
      "[108]\ttrain-mlogloss:0.07065\tval-mlogloss:0.30965\n",
      "[109]\ttrain-mlogloss:0.06987\tval-mlogloss:0.30945\n",
      "[110]\ttrain-mlogloss:0.06902\tval-mlogloss:0.30971\n",
      "[111]\ttrain-mlogloss:0.06819\tval-mlogloss:0.30996\n",
      "[112]\ttrain-mlogloss:0.06746\tval-mlogloss:0.30991\n",
      "[113]\ttrain-mlogloss:0.06664\tval-mlogloss:0.31011\n",
      "[114]\ttrain-mlogloss:0.06601\tval-mlogloss:0.31038\n",
      "[115]\ttrain-mlogloss:0.06529\tval-mlogloss:0.31026\n",
      "[116]\ttrain-mlogloss:0.06447\tval-mlogloss:0.31045\n",
      "[117]\ttrain-mlogloss:0.06377\tval-mlogloss:0.31061\n",
      "[118]\ttrain-mlogloss:0.06316\tval-mlogloss:0.31062\n",
      "[119]\ttrain-mlogloss:0.06240\tval-mlogloss:0.31087\n",
      "[120]\ttrain-mlogloss:0.06189\tval-mlogloss:0.31103\n",
      "[121]\ttrain-mlogloss:0.06129\tval-mlogloss:0.31108\n",
      "[122]\ttrain-mlogloss:0.06058\tval-mlogloss:0.31140\n",
      "[123]\ttrain-mlogloss:0.06011\tval-mlogloss:0.31161\n",
      "[124]\ttrain-mlogloss:0.05948\tval-mlogloss:0.31163\n",
      "[125]\ttrain-mlogloss:0.05874\tval-mlogloss:0.31159\n",
      "[126]\ttrain-mlogloss:0.05820\tval-mlogloss:0.31196\n",
      "[127]\ttrain-mlogloss:0.05774\tval-mlogloss:0.31202\n",
      "[128]\ttrain-mlogloss:0.05722\tval-mlogloss:0.31222\n",
      "[129]\ttrain-mlogloss:0.05667\tval-mlogloss:0.31236\n",
      "[130]\ttrain-mlogloss:0.05620\tval-mlogloss:0.31247\n",
      "[131]\ttrain-mlogloss:0.05564\tval-mlogloss:0.31270\n",
      "[132]\ttrain-mlogloss:0.05527\tval-mlogloss:0.31275\n",
      "[133]\ttrain-mlogloss:0.05460\tval-mlogloss:0.31316\n",
      "[134]\ttrain-mlogloss:0.05414\tval-mlogloss:0.31331\n",
      "[135]\ttrain-mlogloss:0.05358\tval-mlogloss:0.31317\n",
      "[136]\ttrain-mlogloss:0.05304\tval-mlogloss:0.31348\n",
      "[137]\ttrain-mlogloss:0.05260\tval-mlogloss:0.31373\n",
      "[138]\ttrain-mlogloss:0.05207\tval-mlogloss:0.31408\n",
      "[139]\ttrain-mlogloss:0.05147\tval-mlogloss:0.31404\n",
      "[140]\ttrain-mlogloss:0.05097\tval-mlogloss:0.31448\n",
      "[141]\ttrain-mlogloss:0.05050\tval-mlogloss:0.31453\n",
      "[142]\ttrain-mlogloss:0.05003\tval-mlogloss:0.31476\n",
      "[143]\ttrain-mlogloss:0.04955\tval-mlogloss:0.31503\n",
      "[144]\ttrain-mlogloss:0.04907\tval-mlogloss:0.31521\n",
      "[145]\ttrain-mlogloss:0.04861\tval-mlogloss:0.31550\n",
      "[146]\ttrain-mlogloss:0.04816\tval-mlogloss:0.31574\n",
      "FOLD: 3\n",
      "2777 11110\n",
      "[15:48:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.79152\tval-mlogloss:1.80394\n",
      "[1]\ttrain-mlogloss:1.57993\tval-mlogloss:1.60449\n",
      "[2]\ttrain-mlogloss:1.41292\tval-mlogloss:1.44747\n",
      "[3]\ttrain-mlogloss:1.27798\tval-mlogloss:1.31996\n",
      "[4]\ttrain-mlogloss:1.16905\tval-mlogloss:1.21862\n",
      "[5]\ttrain-mlogloss:1.07219\tval-mlogloss:1.12784\n",
      "[6]\ttrain-mlogloss:0.98817\tval-mlogloss:1.05037\n",
      "[7]\ttrain-mlogloss:0.91537\tval-mlogloss:0.98334\n",
      "[8]\ttrain-mlogloss:0.85022\tval-mlogloss:0.92431\n",
      "[9]\ttrain-mlogloss:0.79244\tval-mlogloss:0.87156\n",
      "[10]\ttrain-mlogloss:0.74103\tval-mlogloss:0.82494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11]\ttrain-mlogloss:0.69424\tval-mlogloss:0.78209\n",
      "[12]\ttrain-mlogloss:0.65237\tval-mlogloss:0.74471\n",
      "[13]\ttrain-mlogloss:0.61374\tval-mlogloss:0.71051\n",
      "[14]\ttrain-mlogloss:0.57796\tval-mlogloss:0.67978\n",
      "[15]\ttrain-mlogloss:0.54670\tval-mlogloss:0.65170\n",
      "[16]\ttrain-mlogloss:0.51777\tval-mlogloss:0.62672\n",
      "[17]\ttrain-mlogloss:0.49187\tval-mlogloss:0.60400\n",
      "[18]\ttrain-mlogloss:0.46692\tval-mlogloss:0.58334\n",
      "[19]\ttrain-mlogloss:0.44469\tval-mlogloss:0.56438\n",
      "[20]\ttrain-mlogloss:0.42395\tval-mlogloss:0.54658\n",
      "[21]\ttrain-mlogloss:0.40511\tval-mlogloss:0.53131\n",
      "[22]\ttrain-mlogloss:0.38821\tval-mlogloss:0.51682\n",
      "[23]\ttrain-mlogloss:0.37216\tval-mlogloss:0.50407\n",
      "[24]\ttrain-mlogloss:0.35705\tval-mlogloss:0.49162\n",
      "[25]\ttrain-mlogloss:0.34238\tval-mlogloss:0.48097\n",
      "[26]\ttrain-mlogloss:0.32850\tval-mlogloss:0.47035\n",
      "[27]\ttrain-mlogloss:0.31621\tval-mlogloss:0.46107\n",
      "[28]\ttrain-mlogloss:0.30406\tval-mlogloss:0.45202\n",
      "[29]\ttrain-mlogloss:0.29244\tval-mlogloss:0.44373\n",
      "[30]\ttrain-mlogloss:0.28196\tval-mlogloss:0.43628\n",
      "[31]\ttrain-mlogloss:0.27257\tval-mlogloss:0.42978\n",
      "[32]\ttrain-mlogloss:0.26323\tval-mlogloss:0.42387\n",
      "[33]\ttrain-mlogloss:0.25450\tval-mlogloss:0.41768\n",
      "[34]\ttrain-mlogloss:0.24656\tval-mlogloss:0.41281\n",
      "[35]\ttrain-mlogloss:0.23855\tval-mlogloss:0.40772\n",
      "[36]\ttrain-mlogloss:0.23147\tval-mlogloss:0.40308\n",
      "[37]\ttrain-mlogloss:0.22459\tval-mlogloss:0.39906\n",
      "[38]\ttrain-mlogloss:0.21803\tval-mlogloss:0.39491\n",
      "[39]\ttrain-mlogloss:0.21232\tval-mlogloss:0.39168\n",
      "[40]\ttrain-mlogloss:0.20682\tval-mlogloss:0.38837\n",
      "[41]\ttrain-mlogloss:0.20155\tval-mlogloss:0.38540\n",
      "[42]\ttrain-mlogloss:0.19673\tval-mlogloss:0.38251\n",
      "[43]\ttrain-mlogloss:0.19220\tval-mlogloss:0.38007\n",
      "[44]\ttrain-mlogloss:0.18745\tval-mlogloss:0.37747\n",
      "[45]\ttrain-mlogloss:0.18310\tval-mlogloss:0.37515\n",
      "[46]\ttrain-mlogloss:0.17906\tval-mlogloss:0.37281\n",
      "[47]\ttrain-mlogloss:0.17486\tval-mlogloss:0.37066\n",
      "[48]\ttrain-mlogloss:0.17092\tval-mlogloss:0.36848\n",
      "[49]\ttrain-mlogloss:0.16718\tval-mlogloss:0.36679\n",
      "[50]\ttrain-mlogloss:0.16394\tval-mlogloss:0.36502\n",
      "[51]\ttrain-mlogloss:0.16108\tval-mlogloss:0.36351\n",
      "[52]\ttrain-mlogloss:0.15807\tval-mlogloss:0.36219\n",
      "[53]\ttrain-mlogloss:0.15488\tval-mlogloss:0.36113\n",
      "[54]\ttrain-mlogloss:0.15203\tval-mlogloss:0.36003\n",
      "[55]\ttrain-mlogloss:0.14879\tval-mlogloss:0.35889\n",
      "[56]\ttrain-mlogloss:0.14592\tval-mlogloss:0.35793\n",
      "[57]\ttrain-mlogloss:0.14374\tval-mlogloss:0.35698\n",
      "[58]\ttrain-mlogloss:0.14128\tval-mlogloss:0.35631\n",
      "[59]\ttrain-mlogloss:0.13876\tval-mlogloss:0.35586\n",
      "[60]\ttrain-mlogloss:0.13627\tval-mlogloss:0.35532\n",
      "[61]\ttrain-mlogloss:0.13378\tval-mlogloss:0.35427\n",
      "[62]\ttrain-mlogloss:0.13135\tval-mlogloss:0.35344\n",
      "[63]\ttrain-mlogloss:0.12884\tval-mlogloss:0.35262\n",
      "[64]\ttrain-mlogloss:0.12694\tval-mlogloss:0.35186\n",
      "[65]\ttrain-mlogloss:0.12457\tval-mlogloss:0.35125\n",
      "[66]\ttrain-mlogloss:0.12233\tval-mlogloss:0.35078\n",
      "[67]\ttrain-mlogloss:0.12016\tval-mlogloss:0.35039\n",
      "[68]\ttrain-mlogloss:0.11794\tval-mlogloss:0.34986\n",
      "[69]\ttrain-mlogloss:0.11608\tval-mlogloss:0.34947\n",
      "[70]\ttrain-mlogloss:0.11386\tval-mlogloss:0.34911\n",
      "[71]\ttrain-mlogloss:0.11227\tval-mlogloss:0.34913\n",
      "[72]\ttrain-mlogloss:0.11070\tval-mlogloss:0.34887\n",
      "[73]\ttrain-mlogloss:0.10909\tval-mlogloss:0.34866\n",
      "[74]\ttrain-mlogloss:0.10699\tval-mlogloss:0.34851\n",
      "[75]\ttrain-mlogloss:0.10536\tval-mlogloss:0.34836\n",
      "[76]\ttrain-mlogloss:0.10374\tval-mlogloss:0.34794\n",
      "[77]\ttrain-mlogloss:0.10208\tval-mlogloss:0.34773\n",
      "[78]\ttrain-mlogloss:0.10063\tval-mlogloss:0.34745\n",
      "[79]\ttrain-mlogloss:0.09913\tval-mlogloss:0.34709\n",
      "[80]\ttrain-mlogloss:0.09770\tval-mlogloss:0.34721\n",
      "[81]\ttrain-mlogloss:0.09625\tval-mlogloss:0.34689\n",
      "[82]\ttrain-mlogloss:0.09472\tval-mlogloss:0.34654\n",
      "[83]\ttrain-mlogloss:0.09338\tval-mlogloss:0.34666\n",
      "[84]\ttrain-mlogloss:0.09181\tval-mlogloss:0.34676\n",
      "[85]\ttrain-mlogloss:0.09086\tval-mlogloss:0.34680\n",
      "[86]\ttrain-mlogloss:0.08969\tval-mlogloss:0.34676\n",
      "[87]\ttrain-mlogloss:0.08839\tval-mlogloss:0.34667\n",
      "[88]\ttrain-mlogloss:0.08728\tval-mlogloss:0.34672\n",
      "[89]\ttrain-mlogloss:0.08600\tval-mlogloss:0.34652\n",
      "[90]\ttrain-mlogloss:0.08481\tval-mlogloss:0.34651\n",
      "[91]\ttrain-mlogloss:0.08373\tval-mlogloss:0.34647\n",
      "[92]\ttrain-mlogloss:0.08282\tval-mlogloss:0.34633\n",
      "[93]\ttrain-mlogloss:0.08194\tval-mlogloss:0.34617\n",
      "[94]\ttrain-mlogloss:0.08063\tval-mlogloss:0.34639\n",
      "[95]\ttrain-mlogloss:0.07961\tval-mlogloss:0.34643\n",
      "[96]\ttrain-mlogloss:0.07864\tval-mlogloss:0.34628\n",
      "[97]\ttrain-mlogloss:0.07768\tval-mlogloss:0.34593\n",
      "[98]\ttrain-mlogloss:0.07675\tval-mlogloss:0.34617\n",
      "[99]\ttrain-mlogloss:0.07584\tval-mlogloss:0.34651\n",
      "[100]\ttrain-mlogloss:0.07506\tval-mlogloss:0.34631\n",
      "[101]\ttrain-mlogloss:0.07409\tval-mlogloss:0.34613\n",
      "[102]\ttrain-mlogloss:0.07310\tval-mlogloss:0.34614\n",
      "[103]\ttrain-mlogloss:0.07237\tval-mlogloss:0.34586\n",
      "[104]\ttrain-mlogloss:0.07143\tval-mlogloss:0.34589\n",
      "[105]\ttrain-mlogloss:0.07071\tval-mlogloss:0.34645\n",
      "[106]\ttrain-mlogloss:0.06986\tval-mlogloss:0.34648\n",
      "[107]\ttrain-mlogloss:0.06893\tval-mlogloss:0.34668\n",
      "[108]\ttrain-mlogloss:0.06809\tval-mlogloss:0.34677\n",
      "[109]\ttrain-mlogloss:0.06742\tval-mlogloss:0.34722\n",
      "[110]\ttrain-mlogloss:0.06657\tval-mlogloss:0.34719\n",
      "[111]\ttrain-mlogloss:0.06597\tval-mlogloss:0.34704\n",
      "[112]\ttrain-mlogloss:0.06523\tval-mlogloss:0.34705\n",
      "[113]\ttrain-mlogloss:0.06463\tval-mlogloss:0.34712\n",
      "[114]\ttrain-mlogloss:0.06382\tval-mlogloss:0.34706\n",
      "[115]\ttrain-mlogloss:0.06317\tval-mlogloss:0.34747\n",
      "[116]\ttrain-mlogloss:0.06256\tval-mlogloss:0.34758\n",
      "[117]\ttrain-mlogloss:0.06205\tval-mlogloss:0.34789\n",
      "[118]\ttrain-mlogloss:0.06139\tval-mlogloss:0.34829\n",
      "[119]\ttrain-mlogloss:0.06062\tval-mlogloss:0.34855\n",
      "[120]\ttrain-mlogloss:0.06008\tval-mlogloss:0.34844\n",
      "[121]\ttrain-mlogloss:0.05938\tval-mlogloss:0.34858\n",
      "[122]\ttrain-mlogloss:0.05869\tval-mlogloss:0.34893\n",
      "[123]\ttrain-mlogloss:0.05812\tval-mlogloss:0.34902\n",
      "[124]\ttrain-mlogloss:0.05755\tval-mlogloss:0.34897\n",
      "[125]\ttrain-mlogloss:0.05691\tval-mlogloss:0.34891\n",
      "[126]\ttrain-mlogloss:0.05634\tval-mlogloss:0.34914\n",
      "[127]\ttrain-mlogloss:0.05575\tval-mlogloss:0.34917\n",
      "[128]\ttrain-mlogloss:0.05520\tval-mlogloss:0.34952\n",
      "[129]\ttrain-mlogloss:0.05463\tval-mlogloss:0.35008\n",
      "[130]\ttrain-mlogloss:0.05403\tval-mlogloss:0.35033\n",
      "[131]\ttrain-mlogloss:0.05351\tval-mlogloss:0.35061\n",
      "[132]\ttrain-mlogloss:0.05298\tval-mlogloss:0.35101\n",
      "[133]\ttrain-mlogloss:0.05256\tval-mlogloss:0.35116\n",
      "[134]\ttrain-mlogloss:0.05203\tval-mlogloss:0.35183\n",
      "[135]\ttrain-mlogloss:0.05158\tval-mlogloss:0.35199\n",
      "[136]\ttrain-mlogloss:0.05112\tval-mlogloss:0.35250\n",
      "[137]\ttrain-mlogloss:0.05073\tval-mlogloss:0.35259\n",
      "[138]\ttrain-mlogloss:0.05015\tval-mlogloss:0.35317\n",
      "[139]\ttrain-mlogloss:0.04972\tval-mlogloss:0.35312\n",
      "[140]\ttrain-mlogloss:0.04922\tval-mlogloss:0.35303\n",
      "[141]\ttrain-mlogloss:0.04869\tval-mlogloss:0.35350\n",
      "[142]\ttrain-mlogloss:0.04818\tval-mlogloss:0.35406\n",
      "[143]\ttrain-mlogloss:0.04771\tval-mlogloss:0.35416\n",
      "[144]\ttrain-mlogloss:0.04726\tval-mlogloss:0.35436\n",
      "[145]\ttrain-mlogloss:0.04676\tval-mlogloss:0.35448\n",
      "[146]\ttrain-mlogloss:0.04646\tval-mlogloss:0.35482\n",
      "[147]\ttrain-mlogloss:0.04607\tval-mlogloss:0.35512\n",
      "[148]\ttrain-mlogloss:0.04569\tval-mlogloss:0.35551\n",
      "[149]\ttrain-mlogloss:0.04531\tval-mlogloss:0.35579\n",
      "[150]\ttrain-mlogloss:0.04487\tval-mlogloss:0.35620\n",
      "[151]\ttrain-mlogloss:0.04458\tval-mlogloss:0.35638\n",
      "[152]\ttrain-mlogloss:0.04421\tval-mlogloss:0.35707\n",
      "FOLD: 4\n",
      "2777 11110\n",
      "[15:58:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.79092\tval-mlogloss:1.79852\n",
      "[1]\ttrain-mlogloss:1.58298\tval-mlogloss:1.59701\n",
      "[2]\ttrain-mlogloss:1.42018\tval-mlogloss:1.43895\n",
      "[3]\ttrain-mlogloss:1.28505\tval-mlogloss:1.30950\n",
      "[4]\ttrain-mlogloss:1.17707\tval-mlogloss:1.20677\n",
      "[5]\ttrain-mlogloss:1.08018\tval-mlogloss:1.11362\n",
      "[6]\ttrain-mlogloss:0.99714\tval-mlogloss:1.03534\n",
      "[7]\ttrain-mlogloss:0.92279\tval-mlogloss:0.96417\n",
      "[8]\ttrain-mlogloss:0.85690\tval-mlogloss:0.90277\n",
      "[9]\ttrain-mlogloss:0.79922\tval-mlogloss:0.84845\n",
      "[10]\ttrain-mlogloss:0.74649\tval-mlogloss:0.79894\n",
      "[11]\ttrain-mlogloss:0.69982\tval-mlogloss:0.75611\n",
      "[12]\ttrain-mlogloss:0.65779\tval-mlogloss:0.71693\n",
      "[13]\ttrain-mlogloss:0.61885\tval-mlogloss:0.68114\n",
      "[14]\ttrain-mlogloss:0.58374\tval-mlogloss:0.64895\n",
      "[15]\ttrain-mlogloss:0.55132\tval-mlogloss:0.61968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16]\ttrain-mlogloss:0.52274\tval-mlogloss:0.59340\n",
      "[17]\ttrain-mlogloss:0.49673\tval-mlogloss:0.56975\n",
      "[18]\ttrain-mlogloss:0.47178\tval-mlogloss:0.54741\n",
      "[19]\ttrain-mlogloss:0.44867\tval-mlogloss:0.52726\n",
      "[20]\ttrain-mlogloss:0.42802\tval-mlogloss:0.50952\n",
      "[21]\ttrain-mlogloss:0.40851\tval-mlogloss:0.49265\n",
      "[22]\ttrain-mlogloss:0.39021\tval-mlogloss:0.47719\n",
      "[23]\ttrain-mlogloss:0.37361\tval-mlogloss:0.46278\n",
      "[24]\ttrain-mlogloss:0.35808\tval-mlogloss:0.45024\n",
      "[25]\ttrain-mlogloss:0.34379\tval-mlogloss:0.43831\n",
      "[26]\ttrain-mlogloss:0.33057\tval-mlogloss:0.42809\n",
      "[27]\ttrain-mlogloss:0.31780\tval-mlogloss:0.41796\n",
      "[28]\ttrain-mlogloss:0.30577\tval-mlogloss:0.40892\n",
      "[29]\ttrain-mlogloss:0.29466\tval-mlogloss:0.40021\n",
      "[30]\ttrain-mlogloss:0.28426\tval-mlogloss:0.39249\n",
      "[31]\ttrain-mlogloss:0.27447\tval-mlogloss:0.38550\n",
      "[32]\ttrain-mlogloss:0.26513\tval-mlogloss:0.37954\n",
      "[33]\ttrain-mlogloss:0.25645\tval-mlogloss:0.37333\n",
      "[34]\ttrain-mlogloss:0.24799\tval-mlogloss:0.36775\n",
      "[35]\ttrain-mlogloss:0.24010\tval-mlogloss:0.36237\n",
      "[36]\ttrain-mlogloss:0.23319\tval-mlogloss:0.35779\n",
      "[37]\ttrain-mlogloss:0.22663\tval-mlogloss:0.35372\n",
      "[38]\ttrain-mlogloss:0.22014\tval-mlogloss:0.34945\n",
      "[39]\ttrain-mlogloss:0.21381\tval-mlogloss:0.34577\n",
      "[40]\ttrain-mlogloss:0.20746\tval-mlogloss:0.34197\n",
      "[41]\ttrain-mlogloss:0.20182\tval-mlogloss:0.33880\n",
      "[42]\ttrain-mlogloss:0.19648\tval-mlogloss:0.33616\n",
      "[43]\ttrain-mlogloss:0.19193\tval-mlogloss:0.33366\n",
      "[44]\ttrain-mlogloss:0.18752\tval-mlogloss:0.33124\n",
      "[45]\ttrain-mlogloss:0.18319\tval-mlogloss:0.32896\n",
      "[46]\ttrain-mlogloss:0.17956\tval-mlogloss:0.32685\n",
      "[47]\ttrain-mlogloss:0.17570\tval-mlogloss:0.32511\n",
      "[48]\ttrain-mlogloss:0.17186\tval-mlogloss:0.32333\n",
      "[49]\ttrain-mlogloss:0.16813\tval-mlogloss:0.32107\n",
      "[50]\ttrain-mlogloss:0.16407\tval-mlogloss:0.31927\n",
      "[51]\ttrain-mlogloss:0.16081\tval-mlogloss:0.31785\n",
      "[52]\ttrain-mlogloss:0.15738\tval-mlogloss:0.31656\n",
      "[53]\ttrain-mlogloss:0.15395\tval-mlogloss:0.31482\n",
      "[54]\ttrain-mlogloss:0.15104\tval-mlogloss:0.31332\n",
      "[55]\ttrain-mlogloss:0.14799\tval-mlogloss:0.31189\n",
      "[56]\ttrain-mlogloss:0.14525\tval-mlogloss:0.31083\n",
      "[57]\ttrain-mlogloss:0.14286\tval-mlogloss:0.31006\n",
      "[58]\ttrain-mlogloss:0.14045\tval-mlogloss:0.30935\n",
      "[59]\ttrain-mlogloss:0.13786\tval-mlogloss:0.30850\n",
      "[60]\ttrain-mlogloss:0.13561\tval-mlogloss:0.30748\n",
      "[61]\ttrain-mlogloss:0.13282\tval-mlogloss:0.30657\n",
      "[62]\ttrain-mlogloss:0.13046\tval-mlogloss:0.30592\n",
      "[63]\ttrain-mlogloss:0.12819\tval-mlogloss:0.30474\n",
      "[64]\ttrain-mlogloss:0.12568\tval-mlogloss:0.30395\n",
      "[65]\ttrain-mlogloss:0.12366\tval-mlogloss:0.30340\n",
      "[66]\ttrain-mlogloss:0.12154\tval-mlogloss:0.30261\n",
      "[67]\ttrain-mlogloss:0.11973\tval-mlogloss:0.30209\n",
      "[68]\ttrain-mlogloss:0.11767\tval-mlogloss:0.30151\n",
      "[69]\ttrain-mlogloss:0.11586\tval-mlogloss:0.30099\n",
      "[70]\ttrain-mlogloss:0.11428\tval-mlogloss:0.30060\n",
      "[71]\ttrain-mlogloss:0.11269\tval-mlogloss:0.30050\n",
      "[72]\ttrain-mlogloss:0.11133\tval-mlogloss:0.29991\n",
      "[73]\ttrain-mlogloss:0.10960\tval-mlogloss:0.29923\n",
      "[74]\ttrain-mlogloss:0.10818\tval-mlogloss:0.29874\n",
      "[75]\ttrain-mlogloss:0.10648\tval-mlogloss:0.29842\n",
      "[76]\ttrain-mlogloss:0.10513\tval-mlogloss:0.29785\n",
      "[77]\ttrain-mlogloss:0.10343\tval-mlogloss:0.29786\n",
      "[78]\ttrain-mlogloss:0.10214\tval-mlogloss:0.29749\n",
      "[79]\ttrain-mlogloss:0.10064\tval-mlogloss:0.29679\n",
      "[80]\ttrain-mlogloss:0.09907\tval-mlogloss:0.29644\n",
      "[81]\ttrain-mlogloss:0.09748\tval-mlogloss:0.29616\n",
      "[82]\ttrain-mlogloss:0.09605\tval-mlogloss:0.29620\n",
      "[83]\ttrain-mlogloss:0.09474\tval-mlogloss:0.29583\n",
      "[84]\ttrain-mlogloss:0.09355\tval-mlogloss:0.29555\n",
      "[85]\ttrain-mlogloss:0.09220\tval-mlogloss:0.29565\n",
      "[86]\ttrain-mlogloss:0.09081\tval-mlogloss:0.29558\n",
      "[87]\ttrain-mlogloss:0.08961\tval-mlogloss:0.29538\n",
      "[88]\ttrain-mlogloss:0.08845\tval-mlogloss:0.29523\n",
      "[89]\ttrain-mlogloss:0.08720\tval-mlogloss:0.29505\n",
      "[90]\ttrain-mlogloss:0.08613\tval-mlogloss:0.29505\n",
      "[91]\ttrain-mlogloss:0.08490\tval-mlogloss:0.29473\n",
      "[92]\ttrain-mlogloss:0.08367\tval-mlogloss:0.29480\n",
      "[93]\ttrain-mlogloss:0.08245\tval-mlogloss:0.29475\n",
      "[94]\ttrain-mlogloss:0.08144\tval-mlogloss:0.29476\n",
      "[95]\ttrain-mlogloss:0.08034\tval-mlogloss:0.29462\n",
      "[96]\ttrain-mlogloss:0.07909\tval-mlogloss:0.29455\n",
      "[97]\ttrain-mlogloss:0.07803\tval-mlogloss:0.29470\n",
      "[98]\ttrain-mlogloss:0.07706\tval-mlogloss:0.29452\n",
      "[99]\ttrain-mlogloss:0.07609\tval-mlogloss:0.29484\n",
      "[100]\ttrain-mlogloss:0.07528\tval-mlogloss:0.29492\n",
      "[101]\ttrain-mlogloss:0.07424\tval-mlogloss:0.29521\n",
      "[102]\ttrain-mlogloss:0.07352\tval-mlogloss:0.29485\n",
      "[103]\ttrain-mlogloss:0.07261\tval-mlogloss:0.29489\n",
      "[104]\ttrain-mlogloss:0.07171\tval-mlogloss:0.29488\n",
      "[105]\ttrain-mlogloss:0.07081\tval-mlogloss:0.29492\n",
      "[106]\ttrain-mlogloss:0.06990\tval-mlogloss:0.29502\n",
      "[107]\ttrain-mlogloss:0.06911\tval-mlogloss:0.29542\n",
      "[108]\ttrain-mlogloss:0.06839\tval-mlogloss:0.29543\n",
      "[109]\ttrain-mlogloss:0.06777\tval-mlogloss:0.29541\n",
      "[110]\ttrain-mlogloss:0.06699\tval-mlogloss:0.29562\n",
      "[111]\ttrain-mlogloss:0.06629\tval-mlogloss:0.29585\n",
      "[112]\ttrain-mlogloss:0.06562\tval-mlogloss:0.29583\n",
      "[113]\ttrain-mlogloss:0.06487\tval-mlogloss:0.29573\n",
      "[114]\ttrain-mlogloss:0.06419\tval-mlogloss:0.29571\n",
      "[115]\ttrain-mlogloss:0.06339\tval-mlogloss:0.29596\n",
      "[116]\ttrain-mlogloss:0.06270\tval-mlogloss:0.29589\n",
      "[117]\ttrain-mlogloss:0.06211\tval-mlogloss:0.29592\n",
      "[118]\ttrain-mlogloss:0.06133\tval-mlogloss:0.29600\n",
      "[119]\ttrain-mlogloss:0.06073\tval-mlogloss:0.29622\n",
      "[120]\ttrain-mlogloss:0.06028\tval-mlogloss:0.29620\n",
      "[121]\ttrain-mlogloss:0.05982\tval-mlogloss:0.29628\n",
      "[122]\ttrain-mlogloss:0.05919\tval-mlogloss:0.29615\n",
      "[123]\ttrain-mlogloss:0.05874\tval-mlogloss:0.29628\n",
      "[124]\ttrain-mlogloss:0.05809\tval-mlogloss:0.29651\n",
      "[125]\ttrain-mlogloss:0.05748\tval-mlogloss:0.29686\n",
      "[126]\ttrain-mlogloss:0.05686\tval-mlogloss:0.29707\n",
      "[127]\ttrain-mlogloss:0.05614\tval-mlogloss:0.29736\n",
      "[128]\ttrain-mlogloss:0.05557\tval-mlogloss:0.29715\n",
      "[129]\ttrain-mlogloss:0.05496\tval-mlogloss:0.29737\n",
      "[130]\ttrain-mlogloss:0.05442\tval-mlogloss:0.29727\n",
      "[131]\ttrain-mlogloss:0.05385\tval-mlogloss:0.29756\n",
      "[132]\ttrain-mlogloss:0.05340\tval-mlogloss:0.29776\n",
      "[133]\ttrain-mlogloss:0.05276\tval-mlogloss:0.29788\n",
      "[134]\ttrain-mlogloss:0.05230\tval-mlogloss:0.29808\n",
      "[135]\ttrain-mlogloss:0.05181\tval-mlogloss:0.29821\n",
      "[136]\ttrain-mlogloss:0.05125\tval-mlogloss:0.29835\n",
      "[137]\ttrain-mlogloss:0.05081\tval-mlogloss:0.29844\n",
      "[138]\ttrain-mlogloss:0.05034\tval-mlogloss:0.29852\n",
      "[139]\ttrain-mlogloss:0.04982\tval-mlogloss:0.29869\n",
      "[140]\ttrain-mlogloss:0.04942\tval-mlogloss:0.29910\n",
      "[141]\ttrain-mlogloss:0.04898\tval-mlogloss:0.29915\n",
      "[142]\ttrain-mlogloss:0.04861\tval-mlogloss:0.29930\n",
      "[143]\ttrain-mlogloss:0.04817\tval-mlogloss:0.29928\n",
      "[144]\ttrain-mlogloss:0.04777\tval-mlogloss:0.29956\n",
      "[145]\ttrain-mlogloss:0.04732\tval-mlogloss:0.29980\n",
      "[146]\ttrain-mlogloss:0.04693\tval-mlogloss:0.30001\n",
      "[147]\ttrain-mlogloss:0.04657\tval-mlogloss:0.29997\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "labels = np.array(labels)\n",
    "for i, (train_index, val_index) in enumerate(skf.split(a, labels)):\n",
    "    X_train, X_train_label = a[train_index], labels[train_index]\n",
    "    X_val, X_val_label = a[val_index], labels[val_index]\n",
    "    print('FOLD: {}'.format(str(i)))\n",
    "    print(len(val_index), len(train_index))\n",
    "    dtrain = xgb.DMatrix(X_train, label=X_train_label)\n",
    "    dtest = xgb.DMatrix(X_val, X_val_label)\n",
    "    dpredict = xgb.DMatrix(b)\n",
    "    \n",
    "    param = {'max_depth': 6, 'eta': 0.1, 'eval_metric': 'mlogloss', 'silent': 1, 'objective': 'multi:softprob',\n",
    "             'num_class': 8, 'subsample': 0.8, 'colsample_bytree': 0.85}\n",
    " \n",
    "    evallist = [(dtrain, 'train'), (dtest, 'val')]  \n",
    "    num_round = 300  \n",
    "    boost = xgb.train(param, dtrain, num_round, evallist, early_stopping_rounds=50)\n",
    "    mid = boost.predict(dpredict)\n",
    "    test_result += mid\n",
    "test_result /= 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e55cab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "result = test_result\n",
    "out = []\n",
    "for i in range(len(fileids)):\n",
    "    mid = []\n",
    "    one = result[i].tolist()\n",
    "    mid.append(fileids[i])\n",
    "    mid.extend(one)\n",
    "    out.append(mid)\n",
    "with open(\"word.csv\", 'w',newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"file_id\", \"prob0\", \"prob1\", \"prob2\", \"prob3\", \"prob4\", \"prob5\", \"prob6\", \"prob7\"])\n",
    "    wr.writerows(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
